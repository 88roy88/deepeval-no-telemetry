---
id: confident-ai-guardrails
title: Guardrails for LLMs in Production
sidebar_label: Introduction
---

Confident AI allows you to **easily guard your LLM applications** from accepting malicious inputs and generating unsafe responses with just a single line of code through our suite of guardrails. You can think of these guardrails as binary metrics that evaluate a user input and/or LLM response for malicious intent and unsafe activity.

:::tip
Confident AI offers **10+ guards** designed to detect 20+ LLM vulnerabilities.
:::

## Types of Guardrails

There are 2 types of guardrails: **input guards** are designed to protect against malicious user inputs before they reach your LLM application, while **output guards** evaluate the responses generated by your LLM application before they reach your users.

<div
  style={{
    display: "flex",
    alignItems: "center",
    justifyContent: "center",
  }}
>
  <img
    src="https://confident-bucket.s3.amazonaws.com/guardrails.svg"
    alt="Datasets 1"
    style={{
      height: "auto",
      maxHeight: "800px",
      marginTop: "20px",
      marginBottom: "40px",
    }}
  />
</div>

The number of guards you choose to set up and whether you decide to utilize both types of guards depends on your priorities regarding **latency, cost, and LLM safety**.

:::note
While most guards are strictly for input or output guarding, some guards such as cybersecurity offer **both input and output guarding capabilities**. For these guards, you can specify the `guard_type` during initialization.

```python
from deepeval.guardrails import CybersecurityGuard, GuardType

guard = CyberSecurityGuard(
    guard_type=GuardType.INPUT # or GuardType.OUTPUT
    ...
)
```

:::

## List of Guards

Confident AI provides comprehensive protection with a variety of input and output guards:

### Input Guards

| Input Guard            | Description                                                                                                                         |
| ---------------------- | ----------------------------------------------------------------------------------------------------------------------------------- |
| `CyberSecurityGuard`   | Detects cybersecurity attacks such as SQL injection, shell command injection, or role-based access control (RBAC) violations.       |
| `PromptInjectionGuard` | Detects prompt injection attacks where secondary commands are embedded within user inputs to manipulate the system's behavior.      |
| `JailbreakingGuard`    | Detects jailbreaking attacks disguised as safe inputs, which attempt to bypass system restrictions or ethical guidelines.           |
| `PrivacyGuard`         | Detects inputs that leak private information such as personally identifiable information (PII), API keys, or sensitive credentials. |
| `TopicalGuard`         | Detects inputs that are irrelevant to the context or violate the defined topics or usage boundaries.                                |

### Output Guards

| Output Guard          | Description                                                                                                                      |
| --------------------- | -------------------------------------------------------------------------------------------------------------------------------- |
| `GraphicContentGuard` | Detects outputs containing explicit, violent, or graphic content to ensure safe and appropriate responses.                       |
| `HallucinationGuard`  | Identifies outputs that include factually inaccurate or fabricated information generated by the model.                           |
| `IllegalGuard`        | Detects outputs that promote, support, or facilitate illegal activities or behavior.                                             |
| `ModernizationGuard`  | Detects outputs containing outdated information that do not reflect modern standards or current knowledge.                       |
| `SyntaxGuard`         | Detects outputs that are syntactically incorrect or do not adhere to proper grammatical standards, ensuring clear communication. |
| `ToxicityGuard`       | Detects harmful, offensive, or toxic outputs to maintain respectful and safe interactions.                                       |
| `CyberSecurityGuard`  | Detects outputs that may have been breached or manipulated as a result of a cyberattack.                                         |

## Guarding your LLM Application

### Guard Function

To begin guarding your LLM application, simply import the guard you desire from `deepeval.guardrails`, and call the `guard` function on the user input, LLM response, or both, depending on the specific guard.

```python
from deepeval.guardrails import HallucinationGuard

# Example LLM response to evaluate
llm_response = "The earth is flat"

# Initialize the HallucinationGuard
hallucination_guard = HallucinationGuard()

# Apply the guard to the LLM response
guard_result = hallucination_guard.guard(response=llm_response)
```

:::info
**Incorporate these guards directly into your hosted LLM application** and use the guard results to handle unsafe inputs and outputs effectively.
:::

### Async Mode

Confident AI also supports asynchronous guard calls through the `a_guard` function, allowing you to integrate multiple guards while optimizing for latency.

```python
from deepeval.guardrails import HallucinationGuard, ToxicityGuard
import asyncio

# Example LLM response to evaluate
llm_response = "The earth is flat"

# Initialize multiple guards
hallucination_guard = HallucinationGuard()
toxicity_guard = ToxicityGuard()

# Create asynchronous guard tasks
guard_tasks = [
    hallucination_guard.a_guard(response=llm_response),  # Check for hallucinations
    toxicity_guard.a_guard(response=llm_response)       # Check for toxicity
]

# Run all guard tasks concurrently and gather results
results = await asyncio.gather(*guard_tasks)
```

## Interpreting Guard Results

The `guard()` and `a_guard()` method returns the results of the guardrails, which you can use to retry LLM response generations if necessary. Here's the data structure returned:

```python
class GuardResult(BaseModel):
    breached: bool
    score_breakdown: Union[List, Dict]
```

:::info
The `breached` property will be true if any of the `Guard`s have failed. If you wish to check for the score breakdown of each `Guard`, you can access the `score_breakdown` instead. A `score` of 1 means the `Guard` has been breached, and 0 otherwise.
:::
