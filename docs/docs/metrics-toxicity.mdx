---
id: metrics-toxicity
title: Toxicity
sidebar_label: Toxicity
---

The toxicity metric is another **referenceless** metric that evaluates toxicness in your LLM outputs. This is particularly useful for a fine-tuning use case.

:::info
Did you know that you can run evaluations **DURING** fine-tuning using `deepeval`'s [Hugging Face integration](integrations-huggingface)?
:::

## Required Arguments

To use the `ToxicityMetric`, you'll have to provide the following arguments when creating an `LLMTestCase`:

- `input`
- `actual_output`

## Example

```python
from deepeval.metrics import ToxicityMetric
from deepeval.test_case import LLMTestCase

metric = ToxicityMetric(threshold=0.5)
test_case = LLMTestCase(
    input="What if these shoes don't fit?",
    # Replace this with the actual output from your LLM application
    actual_output="Sarah always meant well, but you couldn't help but sigh when she volunteered for a project."
)

metric.measure(test_case)
print(metric.score)
print(metric.reason)
```

There are three optional parameters when creating a `ToxicityMetric`:

- [Optional] `threshold`: a float representing the maximum passing threshold, defaulted to 0.5.
- [Optional] `model`: a string specifying which of OpenAI's GPT models to use, **OR** [any custom LLM model](metrics-introduction#using-a-custom-llm) of type `DeepEvalBaseLLM`. Defaulted to 'gpt-4-1106-preview'.
- [Optional] `include_reason`: a boolean which when set to `True`, will include a reason for its evaluation score. Defaulted to `True`.

:::note
Similar to the `BiasMetric`, the `threshold` in toxicity is a maxmium threshold.
:::
