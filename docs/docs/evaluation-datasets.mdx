---
id: evaluation-datasets
title: Datasets
sidebar_label: Datasets
---

## Quick Summary

In `deepeval`, am evaluation dataset, or just dataset, is a collection of `LLMTestCase`s. There are two approaches to evaluating datasets in `deepeval`:

1. using `@pytest.mark.parametrize` and `assert_test`
2. using `evaluate`

## Create An Evaluation Dataset

Your original dataset can be in any format, such as CSV or JSON. Let's take JSON for example, the first step is to create a list of `LLMTestCase`s from your original dataset.

```python
from deepeval.test_case import LLMTestCase

original_dataset = [
    {
        "input": "What are your operating hours?",
        # Replace with your LLM application output
        "actual_output": "..."
        "context": [
            "Our company operates from 10 AM to 6 PM, Monday to Friday.",
            "We are closed on weekends and public holidays.",
            "Our customer service is available 24/7.",
        ],
    },
    {
        "input": "Do you offer free shipping?",
        # Replace with your LLM application output
        "actual_output": "..."
        "expected_output": "Yes, we offer free shipping on orders over $50."
    },
    {
        "input": "What is your return policy?",
        # Replace with your LLM application output
        "actual_output": "..."
    },
]

dataset = []
for datapoint in original_dataset:
    input = datapoint.get("input", None)
    actual_output = datapoint.get("actual_output", None)
    expected_output = datapoint.get("expected_output", None)
    context = datapoint.get("context", None)

    test_case = LLMTestCase(
        input=input,
        actual_output=actual_output,
        expected_output=expected_output,
        context=context
    )
    dataset.append(test_case)
```

## Evaluate Your Dataset With Pytest

Before we begin, we highly recommend [logging into Confident AI](https://app.confident-ai.com) to keep track of all evaluation results on the cloud:

```console
deepeval login
```

`deepeval` utilizes the `@pytest.mark.parametrize` decorator to loop through entire datasets.

```python title="test_bulk.py"
from deepeval.test_case import LLMTestCase
from deepeval.metrics.factual_consistency_metric import FactualConsistencyMetric
from deepeval.metric.answer_relevancy_metric import AnswerRelevancyMetric
from deepeval.evaluate import assert_test

dataset = [...]

@pytest.mark.parametrize(
    "test_case",
    dataset,
)
def test_customer_chatbot(test_case: LLMTestCase):
    factual_consistency_metric = FactualConsistencyMetric(minimum_score=0.3)
    answer_relevancy_metric = AnswerRelevancyMetric(minimum_score=0.5)
    assert_test(test_case, [factual_consistency_metric, answer_relevancy_metric])
```

To run several tests cases at once in parallel, use the optional `-n` flag followed by a number (that determines the number of processes that will be used) when executing `deepeval test run`:

```
deepeval test run test_bulk.py -n 3
```

## Evaluate Your Dataset Without Pytest

Alternately, you can use deepeval's `evaluate` function to evaluate datasets. This approach avoids the CLI, but does not allow for parallel test execution.

```python
from deepeval.evaluator import evaluate
from deepeval.metrics.factual_consistency_metric import FactualConsistencyMetric
from deepeval.metric.answer_relevancy_metric import AnswerRelevancyMetric

dataset = [...]

factual_consistency_metric = FactualConsistencyMetric(minimum_score=0.3)
answer_relevancy_metric = AnswerRelevancyMetric(minimum_score=0.5)
evaluate(dataset, [factual_consistency_metric, answer_relevancy_metric])
```
