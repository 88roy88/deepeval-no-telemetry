---
id: confident-ai-llm-monitoring
title: LLM Application Monitoring in Production
sidebar_label: LLM Monitoring in Production
---

## Quick Summary

`deepeval` allows you to **monitor live LLM responses in production** with a single API call. By monitoring responses in production, you can leverage Confident AI to [identify unsatisfactory LLM responses](confident-ai-llm-monitoring-platform), have users and human annotators [leave feedback](confident-ai-human-feedback) on such responses, and incorperate real-world data to [improve your evaluation dataset](confident-ai-llm-monitoring-dataset) over time.

:::tip
Additionally, you may enable [**real-time evaluations**](confident-ai-llm-monitoring-evaluations), which automatically runs under the hood to help you identify preliminary failing responses, as well as [**LLM tracing**](confident-ai-tracing), which allows you to easily debug your LLM applications.
:::

## Monitoring Live Responses

To monitor LLM responses, use the `deepeval.monitor(...)` method in your LLM application to start monitoring responses.

```python
import deepeval

# At the end of your LLM call,
# usually in your backend API handler
deepeval.monitor(
    event_name="Chatbot",
    model="gpt-4",
    input="input",
    response="response"
)
```

There are four mandatory and ten optional parameters when using the `monitor()` function to monitor responses in production:

- `event_name`: type `str` specifying the type of response event monitored. The `event_name` can be thought of as an identifier for different functionalities your LLM application performs.
- `model`: type `str` specifying the name of the LLM model used.
- `input`: type `str`.
- `response`: type `str`.
- [Optional] `retrieval_context`: type `list[str]` that indicates the context that were retrieved in your RAG pipeline.
- [Optional] `additional_data`: type `dict[str, Union[str, dict, Link]]`. [See below](confident-ai-evals-in-production#tracking-additional-custom-data) for further details.
- [Optional] `hyperparameters`: type `dict[str, Union[str, int, float]]`. You can provide a dictionary to specify any additional hyperparamters used to generate the response.
- [Optional] `distinct_id`: type `str` to identify **end users** using your LLM application.
- [Optional] `conversation_id`: type `str` to **group together multiple messages** under a single conversation thread.
- [Optional] `completion_time`: type `float` that indicates how many **seconds** it took your LLM application to complete.
- [Optional] `token_usage`: type `float`
- [Optional] `token_cost`: type `float`
- [Optional] `fail_silently`: type `bool`. You should set this to `False` in development to check if `monitor()` is working properly. Defaulted to `False`.
- [Optional] `raise_expection`: type `bool`. You should set this to `False` in production if you don't want to raise expections in production. Defaulted to `True`.

:::caution
Please do **NOT** provide placeholder values for optional parameters. Leave it blank instead.
:::

The `monitor()` function returns an `response_id` upon a successful API request to Confident's servers, which you can later use to send human feedback regarding a particular LLM response you've monitored.

```python
import deepeval

response_id = deepeval.monitor(...)
```

**Congratulations!** With a few lines of code, `deepeval` will now automatically log all LLM responses in production to Confident AI.

### Logging Custom Hyperparameters

In addition to logging which `model` was used to generate each respective response, you can also associate any custom hyperparameters you wish to each response you're monitoring.

```python
import deepeval

deepeval.monitor(
    ...
    model="gpt-4",
    hyperparameters={
        "prompt template": "...",
        "temperature": 1,
        "chunk size": 500
    }
)
```

:::info
Logging hyperparameters allows you to more easily filter and search for the different responses on Confident AI.
:::

### Logging Additional Custom Data

Similar to hyperparameters, you can easily associate custom additional data for each response.

```python
import deepeval
from deepeval.monitor import Link

deepeval.monitor(
    ...
    additional_data={
        "Example Text": "...",
        # the Link class allows you to access the link directly on Confident AI
        "Example Link": Link(value="https://www.youtube.com"),
        "Example list of Links": [Link(value="https://www.instagram.com")],
        "Example JSON": {"Any Key": "Any Value"}
    },
)
```

Note that you can log either text, a `Link`, list of `Link`s, or any custom dict (as shown in the "Example Json"). Similar to hyperparameter, you can also filter and search for different responses based on the provided custom data.

:::tip Did you know?
Although you can technically log a link as a native string, the `Link(value="...")` class allows you to access said link directly on Confident AI. You should also aim to have your `Link`s start with 'https://...'.
:::

## Responses on Confident AI

Confident offers an observatory to view responses and identify ones where you want to augment your evaluation dataset with. [Click here to learn how](confident-ai-llm-monitoring-platform).

![ok](https://confident-bucket.s3.amazonaws.com/observatory2.svg)
