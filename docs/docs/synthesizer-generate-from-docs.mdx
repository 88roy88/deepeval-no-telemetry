---
id: synthesizer-generate-from-docs
title: Generating Synthetic Data From Documents
sidebar_label: Generate from Docs
---

If your application is a Retrieval-Augmented Generation (RAG) system, generating Goldens from documents can be particularly useful, especially if you already have access to the **documents that make up your knowledge base**. By simply providing these documents, the Synthesizer will automatically handle generating the relevant contexts needed for synthesizing test Goldens.

:::caution
You must install `chromadb` v0.5.3 as an additional dependency when generating from documents. The use of a vector database allows for faster indexing and retrieval of chunks during generation.

```python
pip install chromadb==0.5.3
```

:::

## Generating From Documents

To generate synthetic `Golden`s from documents, simply provide a list of document paths:

```python
from deepeval.synthesizer import Synthesizer

synthesizer = Synthesizer()
synthesizer.generate_goldens_from_docs(
    document_paths=['example.txt', 'example.docx', 'example.pdf'],
)
```

There are one mandatory and eleven optional parameters when using the `generate_goldens_from_docs` method:

- `document_paths`: a list strings, representing the path to the documents from which contexts will be extracted from. Supported documents types include: `.txt`, `.docx`, and `.pdf`.
- [Optional] `include_expected_output`: a boolean which when set to `True`, will additionally generate an `expected_output` for each synthetic `Golden`. Defaulted to `False`.
- [Optional] `max_goldens_per_context`: the maximum number of goldens to be generated per context. Defaulted to 2.
- [Optional] `max_contexts_per_document`: the maximum number of contexts to be generated per document. Defaulted to 3.
- [Optional] `chunk_size`: specifies the size of text chunks (in characters) to be considered for context extraction within each document. Defaulted to 1024.
- [Optional] `chunk_overlap`: an int that determines the overlap size between consecutive text chunks during context extraction. Defaulted to 0.
- [Optional] `num_evolutions`: the number of evolution steps to apply to each generated input. This parameter controls the complexity and diversity of the generated dataset by iteratively refining and evolving the initial inputs. Defaulted to 1.
- [Optional] `evolutions`: a dict with `Evolution` keys and sampling probability values, specifying the distribution of data evolutions to be used. Defaulted to all `Evolution`s with equal probability.
- [Optional] `scenario`: a string, specifying the context or setting in which the inputs are fed into the target LLM.
- [Optional] `task`: a string, representing task or purpose of the target LLM.
- [Optional] `input_format`: a string, representing the ideal structure and format of the inputs to be generated.
- [Optional] `expected_output_format`: a string, representing the ideal structure and format of the expected outputs to be generated.

## Using a Custom Embedding Model

Under the hood, only the `generate_goldens_from_docs()` method uses an embedding model. This is because in order to generate goldens from documents, the `Synthesizer` uses cosine similarity to generate the relevant context needed for data synthesization.

### Using Azure OpenAI

You can use Azure's OpenAI embedding models by running the following commands in the CLI:

```console
deepeval set-azure-openai --openai-endpoint=<endpoint> \
    --openai-api-key=<api_key> \
    --deployment-name=<deployment_name> \
    --openai-api-version=<openai_api_version> \
    --model-version=<model_version>
```

Then, run this to set the Azure OpenAI embedder:

```console
deepeval set-azure-openai-embedding --embedding_deployment-name=<embedding_deployment_name>
```

:::tip Did You Know?
The first command configures `deepeval` to use Azure OpenAI LLM globally, while the second command configures `deepeval` to use Azure OpenAI's embedding models globally.
:::

### Using local LLM models

There are several local LLM providers that offer an OpenAI API compatible endpoint, like Ollama or LM Studio. You can use them with `deepeval` by setting several parameters from the CLI. To configure any of those providers, you need to supply the base URL where the service is running. These are some of the most popular alternatives for base URLs:

- Ollama: http://localhost:11434/v1/
- LM Studio: http://localhost:1234/v1/

For example to use a local model served by Ollama, use the following command:

```console
deepeval set-local-model --model-name=<model_name> \
    --base-url="http://localhost:11434/v1/" \
    --api-key="ollama"
```

Where model_name is one of the LLM that appears when executing `ollama list`.

If you ever wish to stop using your local LLM model and move back to regular OpenAI, simply run:

```console
deepeval unset-local-model
```

Then, run this to set the local Embeddings model:

```console
deepeval set-local-embeddings --model-name=<embedding_model_name> \
    --base-url="http://localhost:11434/v1/" \
    --api-key="ollama"
```

To revert back to the default OpenAI embeddings run:

```console
deepeval unset-local-embeddings
```

For additional instructions about LLM model and embeddings model availability and base URLs, consult the provider's documentation.

### Using Any Custom Model

Alternatively, you can also create a custom embedding model in code by inheriting the base `DeepEvalBaseEmbeddingModel` class. Here is an example of using the same custom Azure OpenAI embedding model but created in code instead using langchain's `langchain_openai` module:

```python
from typing import List, Optional
from langchain_openai import AzureOpenAIEmbeddings
from deepeval.models import DeepEvalBaseEmbeddingModel

class CustomEmbeddingModel(DeepEvalBaseEmbeddingModel):
    def __init__(self):
        pass

    def load_model(self):
        return AzureOpenAIEmbeddings(
            openai_api_version="...",
            azure_deployment="...",
            azure_endpoint="...",
            openai_api_key="...",
        )

    def embed_text(self, text: str) -> List[float]:
        embedding_model = self.load_model()
        return embedding_model.embed_query(text)

    def embed_texts(self, texts: List[str]) -> List[List[float]]:
        embedding_model = self.load_model()
        return embedding_model.embed_documents(texts)

    async def a_embed_text(self, text: str) -> List[float]:
        embedding_model = self.load_model()
        return await embedding_model.aembed_query(text)

    async def a_embed_texts(self, texts: List[str]) -> List[List[float]]:
        embedding_model = self.load_model()
        return await embedding_model.aembed_documents(texts)

    def get_model_name(self):
        "Custom Azure Embedding Model"
```

When creating a custom embedding model, you should **ALWAYS**:

- inherit `DeepEvalBaseEmbeddingModel`.
- implement the `get_model_name()` method, which simply returns a string representing your custom model name.
- implement the `load_model()` method, which will be responsible for returning the model object instance.
- implement the `embed_text()` method with **one and only one** parameter of type `str` as the text to be embedded, and returns a vector of type `List[float]`. We called `embedding_model.embed_query(prompt)` to access the embedded text in this particular example, but this could be different depending on the implementation of your custom model object.
- implement the `embed_texts()` method with **one and only one** parameter of type `List[str]` as the list of strings text to be embedded, and return a list of vectors of type `List[List[float]]`.
- implement the asynchronous `a_embed_text()` and `a_embed_texts()` method, with the same function signature as their respective synchronous versions. Since this is an asynchronous method, remember to use `async/await`.

:::note
If an asynchronous version of your embedding model does not exist, simply reuse the synchronous implementation:

```python
class CustomEmbeddingModel(DeepEvalBaseEmbeddingModel):
    ...
    async def a_embed_text(self, text: str) -> List[float]:
        return self.embed_text(text)
```

:::

Lastly, provide the custom embedding model through the `embedder` parameter when creating a `Synthesizer`:

```python
from deepeval.synthesizer import Synthesizer
...

synthesizer = Synthesizer(embedder=CustomEmbeddingModel())
```

:::tip
If you run into **invalid JSON errors** using custom models, you may want to consult [this guide](guides-using-custom-llms) on using custom LLMs for evaluation, as synthetic data generation also supports pydantic confinement for custom models.
:::

## How Does it Work?

To generate contexts from user-provided documents, the documents are first split into chunks and embedded to form a collection of nodes. Random nodes are then selected, and for each selected node, similar nodes are retrieved and grouped together to create contexts. These contexts are then used to generate synthetic inputs and outputs, which are evolved to form your Golden test dataset.

### Context Generation:

A **token-based text splitter** is used to manage document chunking, meaning the `chunk_size` and `chunk_overlap` parameters do not guarantee exact context sizes. This approach ensures contexts are meaningful and coherent, but might lead to variations in the expected size of each `context`.

<div
  style={{
    display: "flex",
    alignItems: "center",
    justifyContent: "center",
  }}
>
  <img
    src="https://confident-bucket.s3.amazonaws.com/generate_from_docs.svg"
    alt="LangChain"
    style={{
      marginTop: "20px",
      marginBottom: "50px",
      height: "auto",
      maxHeight: "800px",
    }}
  />
</div>

:::caution
The synthesizer will raise an error if `chunk_size` is too large to generate n=`max_contexts_per_document` unique contexts.
:::

### Context Filtering:

Often, chunking contexts can result in trivial or undesirable content, such as a series of white spaces or unwanted characters from document structures. This is why **filtering is important**—to ensure generated inputs are meaningful, relevant, and coherent.

You can also control the quality of generated contexts with DeepEval. You'll remember that when creating the synthesizer, you can optionally define the following 3 parameters:

- `context_quality_threshold`
- `context_similarity_threshold`
- `context_max_retries`.

Similar to input filtering, if contexts do not meet the **required thresholds** after the **maximum retries**, the most recent generation is used. As a result, some generated Goldens in your final evaluation dataset may fall below the minimum context thresholds.

<div
  style={{
    display: "flex",
    alignItems: "center",
    justifyContent: "start",
  }}
>
  <img
    src="https://confident-bucket.s3.amazonaws.com/filtering_context.svg"
    alt="LangChain"
    style={{
      marginTop: "20px",
      marginBottom: "20px",
      height: "auto",
      maxHeight: "800px",
    }}
  />
</div>

:::info
To learn more about how scores for **context quality** and **input quality** are generated, [see this guide](#guides-using-synthesizer).
:::

### Prompt Customization:

You can also customize the input and expected output for each golden by configuring the scenario, task, and input format parameters, which control the synthetic input, while the expected output format controls the synthetic expected output.

For example, the following example demonstrates how to use the `Synthesizer` to generate synthetic Goldens for a **text-to-SQL** use case by leveraging the table schema from the context:

```python
synthesizer.generate_goldens_from_docs(
    document_paths=['example.txt', 'example.docx', 'example.pdf'],
    scenario="Non-technical users trying to query a database using plain English instructions",
    task="Answering text-to-SQL-related queries by querying a database and returning the results to users",
    input_format="English instructions for retrieving information from a database",
    expected_output_format="SQL query based on the given input"
)
```

The scenario, task, and input format parameters are used to enforce the input structure during generation and after all the evolutions have been performed. The output is then generated from the final evolved input, with the output format enforced.

<div
  style={{
    display: "flex",
    alignItems: "center",
    justifyContent: "start",
  }}
>
  <img
    src="https://confident-bucket.s3.amazonaws.com/synthesizer_customization.svg"
    alt="LangChain"
    style={{
      marginTop: "20px",
      marginBottom: "200px0px",
      height: "auto",
      maxHeight: "800px",
    }}
/>

</div>
