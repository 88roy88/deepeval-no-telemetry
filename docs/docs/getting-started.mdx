---
id: getting-started
title: Quick Introduction
sidebar_label: Quick Introduction
---

import Envelope from "@site/src/components/envelope";

**DeepEval** is an open-source evaluation framework for LLMs. DeepEval makes it extremely easy to build
and iterate on LLM (applications) and was built with the following principles in mind:

- Easily "unit test" LLM outputs in a similar way to Pytest.
- Plug-and-use 14+ LLM-evaluated metrics, most with research backing.
- Custom metrics are simple to personalize and create.
- Define evaluation datasets in Python code.
- Real-time evaluations in production (available on Confident AI).

<Envelope />

## Setup A Python Environement

Go to the root directory of your project and create a virtual environement (if you don't already have one). In the CLI, run:

```console
python3 -m venv venv
source venv/bin/activate
```

## Installation

In your newly created virtual environement, run:

```console.log
pip install -U deepeval
```

You can also keep track of all evaluation results by logging onto [Confident AI, an all in one evaluation platform](https://app.confident-ai.com):

```console
deepeval login
```

:::note
**[Contact us](https://calendly.com/jeffreyip-myw/confident-ai-intro-call)** if you're dealing with sensitive data that has to reside in your private VPCs.
:::

## Create Your First Test Case

Run `touch test_example.py` to create a test file in your root directory. Open `test_example.py` and paste in your first test case:

```python title="test_example.py"
from deepeval import assert_test
from deepeval.test_case import LLMTestCase
from deepeval.metrics import AnswerRelevancyMetric

def test_answer_relevancy():
    answer_relevancy_metric = AnswerRelevancyMetric(threshold=0.5)
    test_case = LLMTestCase(
        input="What if these shoes don't fit?",
        # Replace this with the actual output of your LLM application
        actual_output="We offer a 30-day full refund at no extra cost."
    )
    assert_test(test_case, [answer_relevancy_metric])
```

Run `deepeval test run` from the root directory of your project:

```console
deepeval test run test_example.py
```

**Congratulations! Your test case should have passed âœ…** Let's breakdown what happened.

- The variable `input` mimics a user input, and `actual_output` is a placeholder for what your application's supposed to output based on this input.
- The variable `retrieval_context` contains the retrieved context from your knowledge base, and `AnswerRelevancyMetric(threshold=0.5)` is an default metric provided by `deepeval` for you to evaluate your LLM output's relevancy based on the provided retrieval context.
- All metric scores range from 0 - 1, which the `threshold=0.5` threshold ultimately determines if your test have passed or not.

:::info
You'll need to set your `OPENAI_API_KEY` as an enviornment variable before running the `AnswerRelevancyMetric`, since the `AnswerRelevancyMetric` is an LLM-evaluated metric.

To use **ANY** custom LLM of your choice, [check out this part of the docs](evaluation-introduction#using-a-custom-llm).
:::

You can also save test results locally for each test run. Simply set the `DEEPEVAL_RESULTS_FOLDER` environement variable to your relative path of choice:

```console
export DEEPEVAL_RESULTS_FOLDER="./data"
```

## Create Your First Metric

`deepeval` provides two types of LLM evaluation metrics to evaluate LLM outputs: plug-and-use **default** metrics, and **custom** metrics for any evaluation criteria.

### Default Metrics

`deepeval` offers 14+ research backed default metrics covering a wide range of use-cases (such as RAG and fine-tuning). To create a metric, simply import from the `deepeval.metrics` module:

```python
from deepeval.test_case import LLMTestCase
from deepeval.metrics import AnswerRelevancyMetric

test_case = LLMTestCase(input="...", actual_output="...")
metric = AnswerRelevancyMetric(threshold=0.5)

metric.measure(test_case)
print(metric.score, metric.reason)
```

Note that you can run a metric as a standalone or as part of a test run as shown in previous sections.

:::info
All default metrics are evaluated using LLMs, and you can use **ANY** LLM of your choice. For more information, visit the [metrics introduction section.](metrics-introduction)
:::

### Custom Metrics

`deepeval` provides G-Eval, a state-of-the-art LLM evaluation framework for anyone to createa a custom LLM-evaluated metric using natural language. Here's an example:

```python
from deepeval.test_case import LLMTestCase, LLMTestCaseParams
from deepeval.metrics import GEval

test_case = LLMTestCase(input="...", actual_output="...")
metric = GEval(
    name="Correctness",
    criteria="Correctness - determine if the actual output is correct according to the expected output.",
    evaluation_params=[LLMTestCaseParams.ACTUAL_OUTPUT, LLMTestCaseParams.EXPECTED_OUTPUT],
    strict=True
)

metric.measure(test_case)
print(metric.score, metric.reason)
```

Under the hood, `deepeval` first generates a series of evaluation steps, before using these steps in conjuction with information in an `LLMTestCase` for evaluation. For more information, visit the [G-Eval documentation page.](metrics-llm-evals)

:::tip
If you wish to customize your metrics a bit more, you can choose to implement your own metric. You can find a full tutorial [here](metrics-custom), but here's a quick example of how you can create a metric that is **NOT** evaluated using LLMs:

```python
from deepeval.scorer import Scorer
from deepeval.metrics import BaseMetric

class RougeMetric(BaseMetric):
    def __init__(self, threshold: float = 0.5):
        self.threshold = threshold
        self.scorer = Scorer()

    def measure(self, test_case: LLMTestCase):
        self.score = scorer.rouge_score(
            prediction=test_case.actual_output,
            target=test_case.expected_output,
            score_type="rouge1"
        )
        self.success = self.score >= self.threshold
        return self.score

    # Async implementation of measure(). If async version for
    # scoring method does not exist, just reuse the measure method.
    async def a_measure(self, test_case: LLMTestCase):
        return self.measure(test_case)

    def is_successful(self):
        return self.success

    @property
    def __name__(self):
        return "Rouge Metric"
```

You'll notice that although not documented, `deepeval` additionally offers a `scorer` module for more traditional NLP scoring method and can be found [here.](https://github.com/confident-ai/deepeval/blob/main/deepeval/scorer/scorer.py)

You can also create a custom metric to combine several different metrics into one. For example. combining the `AnswerRelevancyMetric` and `FaithfulnessMetric` to test whether an LLM output is both relevant and faithful (ie. not hallucinating).

:::

**Two things to note:**

- Custom metrics requires a `threshsold` as a passing criteria. In the case of our `LengthMetric`, the passing criteria was whether the `max_length` of `actual_output` is greater than 10.
- We removed `retrieval_context` in `test_length` since it was irrelevant to evaluating output length. However `input` and `actual_output` is always mandatory when creating an `LLMTestCase`.

## Evaluate Several Metrics At Once

To avoid redundant code, `deepeval` offers an easy way to apply as many metrics as you wish for a single test case.

```python title="test_example.py"
...

def test_everything():
    assert_test(test_case, [answer_relevancy_metric, coherence_metric, length_metric])
```

In this scenario, `test_everything` only passes if all metrics are passing. Run `deepeval test run` again to see the results:

```console
deepeval test run test_example.py
```

:::info
`deepeval` optimizes evaluation speed by running all metrics for each test case concurrently.
:::

## Create Your First Dataset

A dataset in `deepeval`, or more specifically an evaluation dataset, is simply a collection of `LLMTestCases` and/or `Goldens`.

:::note
We're not going to dive into what a `Golden` is here, but it is an important concept if you're looking to generate LLM outputs at evlauation time. To learn more about `Golden`s, [click here.](evaluation-dataset#with-goldens)
:::

To create a dataset, simply initialize an `EvaluationDataset` with a list of `LLMTestCase`s or `Golden`s:

```python
from deepeval.test_case import LLMTestCase
from deepeval.dataset import EvaluationDataset

first_test_case = LLMTestCase(input="...", actual_output="...")
second_test_case = LLMTestCase(input="...", actual_output="...")

dataset = EvaluationDataset(test_cases=[first_test_case, second_test_case])
```

Then, using `deepeval`'s Pytest integration, you can utilize the `@pytest.mark.parametrize` decorator to loop through and evaluate your dataset.

```python title="test_dataset.py"
import pytest
from deepeval import assert_test
from deepeval.metrics import AnswerRelevancyMetric
...

# Loop through test cases using Pytest
@pytest.mark.parametrize(
    "test_case",
    dataset,
)
def test_customer_chatbot(test_case: LLMTestCase):
    answer_relevancy_metric = AnswerRelevancyMetric(threshold=0.5)
    assert_test(test_case, [answer_relevancy_metric])
```

:::tip
You can also evaluate entire datasets without going through the CLI (if you're in a notebook environment):

```python
from deepeval import evaluate
...

evaluate(dataset, [answer_relevancy_metric])
```

:::

Additionally, you can run test cases in parallel by using the optional `-n` flag followed by a number (that determines the number of processes that will be used) when executing `deepeval test run`:

```
deepeval test run test_dataset.py -n 2
```

Or repeat evaluating each test case how ever many times you want through the `-r` flag:

```
deepeval test run test_dataset.py -r 2
```

:::info
Visit the [datasets section](evaluation-datasets) to learn about everything an `EvaluationDataset` has to offer.
:::

## Using Confident AI

If you have reached this point, you've likely ran `deepeval test run` multiple times. To keep track of all future evaluation results created by `deepeval`, login to **[Confident AI](https://app.confident-ai.com/auth/signup)** by running the following command:

```console
deepeval login
```

**Confident AI** is the platform powering `deepeval`, and allows anyone to:

- safeguard against breaking changes in CI/CD pipelines
- compare hyperparameters, such as model and prompt templates used
- centralize evaluation datasets on the cloud
- run real-time evaluations in production

:::tip
[Click here](confident-ai-introduction) for the full documentation on using Confident AI with `deepeval`.
:::

Follow the instructions displayed on the CLI to create an account, get your Confident API key, and paste it in the CLI.

Once you've pasted your Confident API key in the CLI, execute the previously created test file once more:

```console
deepeval test run test_example.py
```

### Analyze Test Runs

You should now see a link being returned upon test completion. Paste it in your browser to view results.

![ok](https://d2lsxfc3p6r9rv.cloudfront.net/test-summary.png)

### View Individual Test Cases

You can also view individual test cases for enhanced debugging:

![ok](https://d2lsxfc3p6r9rv.cloudfront.net/confident-test-cases.png)

### Compare Hyperparameters

To log hyperparameters (such as prompt templates used) for your LLM application, paste in the following code in `test_example.py`:

```python title="test_example.py"
import deepeval
...

# Just an example of prompt_template
prompt_template = """You are a helpful assistant, answer the following question in a non-judgemental tone.

Question:
{question}
"""

# Although the values in this example are hardcoded,
# you should ideally pass in variables to keep things dynamic
@deepeval.log_hyperparameters(model="gpt-4", prompt_template=prompt_template)
def hyperparameters():
    # Any additional hyperparameters you wish to keep track of
    return {
        "chunk_size": 500,
        "temperature": 0
    }
```

Execute `deepeval test run test_example.py` again to start comparing hyperparmeters for each test run.

![ok](https://d2lsxfc3p6r9rv.cloudfront.net/compare-hyperparameters.png)

## Full Example

You can find the full example [here on our Github](https://github.com/confident-ai/deepeval/blob/main/examples/getting_started/test_example.py).
