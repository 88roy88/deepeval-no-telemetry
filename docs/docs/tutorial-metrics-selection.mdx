---
id: tutorial-metrics-selection
title: Selecting Metrics based on your Evaluation Criteria
sidebar_label:  Criteria-based Metrics Selection
---

Once you have a clearly defined evaluation criteria, selecting metrics becomes significantly easier. In some cases, you may find **existing metrics** in DeepEval that already match your criteria. In others, you'll need to create **custom metrics** to address your unique evaluation needs.  

:::tip
DeepEval provides [14+ metrics](metrics-introduction) to help you evaluate your LLM. **Familiarizing yourself with these metrics** can help you choose the ones that best align with your evaluation criteria.
:::

## Selecting Metrics
In this section, we’ll be selecting the **LLM evaluation metrics** for our medical chatbot based on the evaluation criteria we've established in the previous section. Let’s quickly revisit these criteria:

1. **Directly addressing the user:** The chatbot should directly address users' requests
2. **Providing accurate diagnoses:** Diagnoses must be reliable and based on the provided symptoms 

### 1. Answer Relevancy

Let's start with our first metric, which will evaluate our medical chatbot against our first criterion:
```
Criteria 1: The medical chatbot should address the user directly.
```
Currently, our chatbot sometimes fails to directly address user queries, instead taking the lead in the conversation—for example, asking for appointment details instead of focusing on diagnosing the patient. This results in responses that only tangentially address the user's input. To address this, we should be evaluating **how relevant the chatbot's responses are to the user query**.

Fortunately, DeepEval provides an out-of-the-box `AnswerRelevancy` metric, which evaluates exactly this: how closely the LLM's output aligns with the input. 

:::info
The `AnswerRelevancyMetric` uses an LLM to extract all statements from the `actual_output` and then classifies each statement's relevance to the `input` using the same LLM.
:::

### 2. Contextual Relevancy 

Our next metric addresses the inaccuracies in patient diagnoses. The chatbot's failure to deliver accurate diagnoses in some example interactions suggests that our **RAG tool needs improvement**.

```
Criteria 2: The chatbot should provide accurate diagnoses based on the given symptoms.
```

This is because the RAG engine is responsible for **retrieving relevant medical information from our knowledge base** to support patient diagnoses. To address this, we need to evaluate the retrieval process, specifically whether the retrieved information is relevant to the user query.

DeepEval's `ContextualRelevancy` metric is well-suited for this task. It assesses the overall relevance of the information provided in your `retrieval_context` for a given `input`.

:::tip  
DeepEval offers a total of **5 RAG metrics** to evaluate your RAG pipeline. To learn more about selecting the right metrics for your use case, check out this [in-depth guide on RAG evaluation](guides-rag-evaluation).
:::


### 3. Custom Metric - Answer Correctness

Our final metric will also address criterion 2 and focus on evaluating **diagnosis accuracy**.

```
Criteria 2: The chatbot should provide accurate diagnoses based on the given symptoms.
```

While improving the retrieval process (as discussed previously) can certainly help address this issue, the LLM may still make an incorrect diagnosis even when provided with sufficient medical knowledge. Therefore, we'll be defining an `AnswerCorrectness` metric using `G-Eval` to directly evaluate our chatbot's diagnoses by comparing them against reference ground-truths.

:::note
G-Eval is a **custom metric framework** that enables users to leverage LLMs for evaluating outputs based on their own tailored evaluation criteria. 
:::

Now that we've selected our three metrics, let's define them in code:

## Defining Metrics in DeepEval

To define our **Answer Relevancy**, **Contextual Relevancy**, and custom **G-Eval** metric for answer correctness, you'll first need to install DeepEval. Run the following command in your CLI:

```bash
pip install deepeval
```

### Defining Default Metrics 
Let's begin by defining the Answer Relevancy and Contextual Relevancy metrics, which is as simple as importing and instantiating their respective classes.

```python
from deepeval.metrics import (
    AnswerRelevancyMetric,
    ContextualRelevancyMetric
)

answer_relevancy_metric = AnswerRelevancyMetric()
contextual_relevancy_metric = ContextualRelevancyMetric()
```

### Defining a Custom Metric

Next, we'll define our custom G-Eval metric for answer correctness. This involves specifying the name of the metric, the evaluation criteria, and the parameters to evaluate. In this case, we're assessing the LLM's `actual_output` (diagnosis results in LLM response) based on the `expected_output` (ground truth diagnosis results):


```python
from deepeval.text_case_import import LLMTestCaseParams
from deepeval.metrics import GEval

criteria = "Determine whether the actual output is factually correct based on the expected output."

overdiagnosis_metric = GEval(
    name="Answer Correctness",
    criteria=criteria,
    evaluation_params={
        "input": LLMTestCaseParams.ACTUAL_OUTPUT,
        "actual_output": LLMTestCaseParams.EXPECTED_OUTPUT,
    }
)
```

:::info
**G-Eval is a two-step algorithm** that first uses chain-of-thought reasoning (CoTs) to generate a series of evaluation steps based on the specified `criteria`. It then applies these steps to assess the parameters provided in an `LLMTestCase` and calculate the final score.
:::

With the evaluation criteria defined and metrics selected, we can now begin preparing our evaluation dataset in the next section.

