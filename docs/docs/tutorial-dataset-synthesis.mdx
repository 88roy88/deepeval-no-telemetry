---
id: tutorial-dataset-synthesis
title: Generating Synthetic Data
sidebar_label: Generating Synthetic Data
---


## Quick Summary

If you wish to evaluate your LLM application at scale, you can choose to curate your own dataset or synthetically generate an evaluation dataset. [Manually generating test data is time-consuming and often times not comprehensive](https://www.confident-ai.com/blog/the-definitive-guide-to-synthetic-data-generation-using-llms). Therefore, we’ll be generating a **synthetic evaluation dataset** to evaluate our medical chatbot at scale.

:::tip
Synthetic data should closely **mimic your users' interactions** with your LLM application, from typical use cases to unique edge cases.
:::

DeepEval's synthesizer offers a fast and easy way to generate high-quality goldens for your evaluation datasets in just a few lines of code. We'll be using the following 2 methods to generate our test data.

- **Generating from Documents**
- **Generating from Scratch**

:::info
**Goldens** in DeepEval are similar to `LLMTestCases` but do not require an actual output and retrieval context, which are computed at evaluation time.
:::

## Generating Synthetic Data from Documents

### 1. Defining Style Configuration

Let's begin by generating synthetic data for a typical use case for our medical chatbot: **patients seeking diagnosis**. We'll first need to define the styling configurations that will allow us to mimic this user behaviour.

:::tip
You can optionally **customize the output style and format** of any `input` and/or `expected_output` in your synthetic goldens, by configuring a `StylingConfig` object, which will be passed into your `Synthesizer`.
:::

```python
from deepeval.synthesizer.config import StylingConfig

styling_config = StylingConfig(
    expected_output="Ensure the output resembles a medical chatbot tasked with diagnosing a patient’s illness. It should pose additional questions if the details are inadequate or provide a diagnosis when the input is sufficiently detailed.",
    input_format="Mimic the kind of queries or statements a patient might share with a medical chatbot when seeking a diagnosis.",
    task="The chatbot acts as a specialist in medical diagnosis, integrated with a patient scheduling system. It manages tasks in a sequence to ensure precise and effective appointment setting and diagnosis processing.",
    scenario="Non-medical patients describing symptoms to seek a diagnosis.",
)
```

Given that our knowledge base is essentially a collection of medical data, we need to ensure that the **inputs** accurately reflect individuals seeking medical diagnosis.

We'll also need to align our **expected output** with our LLM's expected behaviour, namely providing correct diagnoses based on the symptoms presented or asking for clarifying symptoms.

### 2. Defining Context Construct Configuration

When generating goldens from documents, the default `max_contexts_per_document` is set to 3. However, given our extensive knowledge base, we’ll need to generate significantly more goldens. Fortunately, Deepeval allows us to adjust this configuration through the `ContextConstructionConfig` object.

```python
from deepeval.synthesizer.config import ContextConstructionConfig

context_config = ContextConstructionConfig(
    max_contexts_per_document=10
)
```

:::tip
This configuration also enables you to specify a custom `critic_model`, define chunking parameters such as `chunk_size` and `chunk_overlap`, and control content quality through pre-set filters.
:::

### 3. Goldens generation

With our configurations defined, let’s finally begin **generating synthetic goldens**. You can generate as many `goldens_per_context` as you’d like. For this tutorial, we’ll set this parameter to 2, as coverage across different contexts is more important.

:::note
We'll be generating our goldens through DeepEval's `EvaluationDataset`, but this can also be accomplished with the `Synthesizer` directly.
:::

```python
dataset=EvaluationDataset()
dataset.generate_goldens_from_docs(
  max_goldens_per_context=2,
  document_paths=["./synthesizer_data/encyclopedia.pdf"],
  context_construction_config=context_config
)
```

### 6. Exploring Additional Generation Configurations

You may want to explore additional styling or quality configurations for your dataset generation. This allows you to create a dataset that is **diverse and include edge cases**.

#### Ambiguous Inputs

For example, you may want to generate synthetic goldens where the user input describes borderline symptoms—sufficient to narrow down the options but not enough to make a definitive diagnosis without asking additional clarifying questions. This helps evaluate how your medical chatbot handles such ambiguous inputs.

```python
styling_config_ambiguous = StylingConfig(
    expected_output="Provide a cautious and detailed response to borderline or ambiguous symptoms. The chatbot should ask clarifying questions when necessary to avoid making unsupported conclusions.",
    input_format="Simulate user inputs that describe borderline symptoms, where the details are vague or insufficient for a definitive diagnosis.",
    task="The chatbot acts as a specialist in medical diagnosis, integrated with a patient scheduling system. It manages tasks in a sequence to ensure precise and effective appointment setting and diagnosis processing.",
    scenario="Non-medical patients describing symptoms that are vague or ambiguous, requiring further clarification from the chatbot."
)
```

:::info
In medical use cases, false positives are generally preferred over false negatives. It's essential to consider your **specific use case and the expected behavior** of your LLM application when exploring different styling configurations.
:::

#### Challenging User Scenarios
In another scenario, you may want to test how your chatbot reacts to users who are rude or in a hurry. This allows you to evaluate its ability to maintain professionalism and provide effective responses under challenging circumstances. Below is an example styling configuration:


```python
styling_config_challenging = StylingConfig(
    expected_output="Respond politely and remain professional, even when the user is rude or impatient. The chatbot should maintain a calm and empathetic tone while providing clear and actionable advice.",
    input_format="Simulate users being rushed, frustrated, or disrespectful in their queries.",
    task="The chatbot acts as a specialist in medical diagnosis, integrated with a patient scheduling system. It manages tasks in a sequence to ensure precise and effective appointment setting and diagnosis processing.",
    scenario="Users who are impatient, stressed, or rude but require medical assistance and advice."
)
```

#### High Quality Configurations

For high-priority use cases, you may want to define higher-quality filters to ensure that the generated dataset meets rigorous standards. While this approach might cost more to generate the same number of goldens, the trade-off can be invaluable for critical applications. Here’s an example of how you can define such quality filters:

```python
from deepeval.synthesizer.config import FiltrationConfig
from deepeval.synthesizer import Synthesizer

filtration_config = FiltrationConfig(
    critic_model="gpt-4o",  # Model used to assess the quality of generated data
    synthetic_input_quality_threshold=0.8  # Minimum acceptable quality score for generated inputs
)
synthesizer = Synthesizer(filtration_config=filtration_config)
```
You might want to push different evaluation datasets for various configurations to the platform. To do so, simply push the new dataset under a different name:

```python
dataest.push(alias="Ambiguous Synthetic Test")
```

## Generating Synthetic Data from Scratch

### 1. Defining Style Configuration

Similar to generating from documents, you'll want to **customize the output style and format** of any `input` and/or `expected_output` when generating synthetic goldens from scratch. When generating from scratch, your creativity is your limit. You can test your LLM for any interaction you can foresee. In the example below, we'll define user inputs to try to book an appointment by providing name information and email.

```python
from deepeval.synthesizer.config import StylingConfig

styling_config = StylingConfig(
    input_format="User inputs including name and email information for appointment booking.",
    expected_output_format="Structured chatbot response to confirm appointment details.",
    task="The chatbot acts as a specialist in medical diagnosis, integrated with a patient scheduling system. It manages tasks in a sequence to ensure precise and effective appointment setting and diagnosis processing.",
    scenario="Non-medical patients describing symptoms to seek a diagnosis."
)
```

Alternatively, you can also just make the patient say a greeting to the chatbot.

```python
styling_config = StylingConfig(
    input_format="Simple greeting from the user to start interaction with the chatbot.",
    expected_output_format="Chatbot's friendly acknowledgment and readiness to assist.",
    task="The chatbot acts as a specialist in medical diagnosis, integrated with a patient scheduling system. It manages tasks in a sequence to ensure precise and effective appointment setting and diagnosis processing.",
    scenario="A patient greeting the chatbot to initiate an interaction."
)
```

### 2. Generating the Goldens

The next step is to simply initialize your synthesizer with the styling configurations and push the dataset to Confident AI for review.

```python
from deepeval.synthesizer import Synthesizer

# Initialize synthesizer for appointment booking
synthesizer = Synthesizer(styling_config=styling_config)
synthesizer.generate_goldens_from_scratch(num_goldens=25)
dataset.push("Synthetic Test from Scratch")
```

:::info
The reason why the expected output format works better than relying on your LLM directly is that it is hyper-focused on one task or topic. In contrast, a typical LLM application operates with a longer system prompt and may even be fine-tuned, making it less focused on handling very specific queries with precision.
:::