---
id: legal-doc-summarizer-running-an-evaluation
title: Running an Evaluation
sidebar_label: Running an Evaluation
---

Before running evaluations, we need to **construct a dataset** with the documents we want to summarize and generate summaries for using our LLM summarizer. This will allow us to apply our metrics directly to the dataset when running our evaluations.

:::caution important
You'll want to login to Confident AI before running an evaluation to enable data persistence.

```python
deepeval login
```

:::

## Constructing a Dataset

If you're building a document summarizer, you likely have a folder of PDFs ready to be processed. First, you'll want to parse these PDFs into strings that can be passed into your LLM summarizer.

```python
import os
import PyPDF2

def extract_text(pdf_path):
    """Extract text from a PDF file."""
    with open(pdf_path, "rb") as file:
        reader = PyPDF2.PdfReader(file)
        text = "\n".join(page.extract_text() for page in reader.pages if page.extract_text())
    return text

# Replace with your folder containing PDFs
pdf_folder = "path/to/pdf/folder"
documents = []  # List to store extracted document strings

# Iterate over PDF files in the folder
for pdf_file in os.listdir(pdf_folder):
    if pdf_file.endswith(".pdf"):
        pdf_path = os.path.join(pdf_folder, pdf_file)
        document_text = extract_text(pdf_path)
        documents.append(document_text)  # Store extracted text
```

Next, we'll call our legal document summarizer `llm.summarize()` on the extracted document texts to generate the summaries for our evaluation dataset. You should replace this function with your actual summarizer.

```python
from deepeval.test_case import LLMTestCase
from deepeval.dataset import EvaluationDataset
from some_llm_library import llm  # Replace with the actual LLM library

# Convert document strings to test cases with LLM summaries
test_cases = [LLMTestCase(input=doc, actual_output=llm.summarize(doc)) for doc in documents]

# Create the evaluation dataset
dataset = EvaluationDataset(test_cases=test_cases)
```

:::info
An `EvaluationDataset` consists of a series of test cases. Each test case contains an `input`, which represents the document we feed into the summarizer, and the `actual_output`, which is the summary generated by the LLM. [More on test cases here](/docs/evaluation-test-cases).
:::

Keep in mind that, for the sake of this tutorial, our `EvaluationDataset` consists of 5 test cases, and our first `test_case` corresponds to the service agreement we inspected when we first [defined our evaluation criteria](legal-doc-summarizer-defining-a-summarization-criteria).

```python
print(dataset.test_cases[0].input)
#CONTRACT FOR SERVICES...
print(dataset.test_cases[0].actual_output)
#This agreement establishes...
print(len(dataset.test_cases))
# 5
```

Now that the dataset is ready, we can finally begin running our first evaluation.

## Running an Evaluation

To run an evaluation, first login to Confident AI.

```
deepeval login
```

Then, pass the metrics we defined in the [previous section](legal-doc-summarizer-selecting-your-metrics) along with the dataset we created into the `evaluate` function.

```python
from deepeval.evaluate

evaluate(dataset, metrics=[concision_metric, completeness_metric])
```

:::tip
The `evaluate` function offers flexible customization for how you want to run evaluations, allowing you to control concurrency for asynchronous operations and manage error handling. Learn more about these options [here](#).
:::

## Analyzing your Test Report

Once your evaluation is complete, you'll be redirected to a Confident AI page displaying the testing report for the five document summaries we defined and evaluated earlier. Each test case includes its status (pass or fail), input (document), and actual output (summary).
