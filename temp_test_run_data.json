{"deployment": false, "testCases": [{"name": "test_length_metric", "input": "placeholder", "actualOutput": "This is a long sentence that is more than 3 letters", "success": true, "metricsMetadata": [{"metric": "Latency", "score": 1.0, "threshold": 10.0, "success": true}], "runDuration": 2.1499989088624716e-05, "latency": 8.3}, {"name": "test_customer_chatbot[test_case0]", "input": "How many cars did Tesla sell?", "actualOutput": "I don't have access to that data", "expectedOutput": "578 in the year of 2022.", "success": false, "metricsMetadata": [{"metric": "Coherence", "score": 0.1242689897104402, "threshold": 0.5, "success": false, "reason": "This metric looking good!"}], "runDuration": 7.458002073690295e-06, "context": ["I don't know"]}, {"name": "test_customer_chatbot[test_case1]", "input": "What's the refund policy", "actualOutput": "I don't know", "expectedOutput": "I don't know", "success": false, "metricsMetadata": [{"metric": "Coherence", "score": 0.2666893953970654, "threshold": 0.5, "success": false, "reason": "This metric looking good!"}], "runDuration": 7.124996045604348e-06, "context": ["I don't know"]}, {"name": "test_customer_chatbot[test_case2]", "input": "How many suitcases can the trunk of a Cybertruck fit?", "actualOutput": "To my knowledge, a cybertruck can fit up to 3 medium sized suitcases.", "expectedOutput": "Four", "success": false, "metricsMetadata": [{"metric": "Coherence", "score": 0.12683304885975322, "threshold": 0.5, "success": false, "reason": "This metric looking good!"}], "runDuration": 4.582980182021856e-06, "context": ["3"]}, {"name": "test_customer_chatbot[test_case3]", "input": "Who is the current president of the united states?", "actualOutput": "Biden", "expectedOutput": "Joe Biden", "success": true, "metricsMetadata": [{"metric": "Coherence", "score": 0.536845362348648, "threshold": 0.5, "success": true, "reason": "This metric looking good!"}], "runDuration": 4.0000013541430235e-06, "context": ["Joe Biden is the current president"]}, {"name": "test_hallucination_metric_2", "input": "placeholder", "actualOutput": "Python is a programming language.", "success": false, "metricsMetadata": [{"metric": "Hallucination", "score": 1.0, "threshold": 0.6, "success": false, "reason": "The score is 1.00 because the actual output directly contradicts the provided context regarding Python's status as a programming language, indicating a complete deviation from the facts presented in the context.", "evaluationModel": "gpt-4-0125-preview"}], "runDuration": 4.936658249993343, "latency": 0.2, "cost": 1.0, "context": ["Python is NOT a programming language."]}, {"name": "test_hallucination_metric_3", "input": "placeholder", "actualOutput": "Python is a programming language.", "success": false, "metricsMetadata": [{"metric": "Hallucination", "score": 1.0, "threshold": 0.6, "success": false, "reason": "The score is 1.00 because the actual output directly contradicts the provided context by misidentifying Python as something other than a programming language, indicating a complete departure from the factual information given.", "evaluationModel": "gpt-4-0125-preview"}], "runDuration": 4.42829695797991, "latency": 13.0, "cost": 0.1, "context": ["Python is a snake."]}, {"name": "test_cost_metric", "input": "...", "actualOutput": "...", "success": true, "metricsMetadata": [{"metric": "Cost", "score": 12.0, "threshold": 12.0, "success": true}], "runDuration": 0.00047237498802132905, "cost": 12.0}, {"name": "test_latency_metric", "input": "...", "actualOutput": "...", "success": true, "metricsMetadata": [{"metric": "Latency", "score": 8.3, "threshold": 12.0, "success": true}], "runDuration": 0.0019009170064236969, "latency": 8.3}], "metricScores": [], "configurations": {}}