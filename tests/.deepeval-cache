{"testCasesLookupMap": {"{\"context\": [\"I love coffee\"], \"expected_output\": \"this is a mocha\", \"input\": \"What is this\", \"retrieval_context\": [\"I love coffee\"]}": {"name": "test_everything_2", "input": "What is this", "actualOutput": "this is a latte", "expectedOutput": "this is a mocha", "success": true, "metricsMetadata": [{"strict_mode": false, "metric": "Answer Relevancy", "score": 1.0, "threshold": 0.7, "success": true, "evaluation_model": "gpt-4-0125-preview", "reason": "The score is 1.00 because the response fully and accurately addresses the question without including any irrelevant information. Great job on maintaining focus and relevancy!", "include_reason": true}], "runDuration": 0.0029867999983252957, "context": ["I love coffee"], "retrievalContext": ["I love coffee"]}, "{\"context\": [\"I love coffee\"], \"expected_output\": \"this is a mocha\", \"input\": \"What is this?\", \"retrieval_context\": [\"I love coffee\"]}": {"name": "test_everything", "input": "What is this?", "actualOutput": "this is a latte", "expectedOutput": "this is a mocha", "success": true, "metricsMetadata": [{"strict_mode": false, "metric": "Answer Relevancy", "score": 1.0, "threshold": 0.5, "success": true, "evaluation_model": "gpt-4-0125-preview", "reason": "The score is 1.00 because the response directly addresses the input without any irrelevant statements. Great job on maintaining focus and relevancy!", "include_reason": true}], "runDuration": 0.0037870000014663674, "context": ["I love coffee"], "retrievalContext": ["I love coffee"]}}, "hyperparameters": {"chunk_size": "600", "temperature": "1"}}